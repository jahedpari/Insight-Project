{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the Domgraphic of Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we want to train a classifier to predict the target audience of a product, if the end consumer of a product is men, women, kids, baby or if it is unisex.\n",
    "Our data include product information such as title, type, vendor name and product URL.\n",
    "\n",
    "In the first step let's load our data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "inputFile = \"../data/cleaned/outputFile2-0-1000000.csv\"\n",
    "df = pd.read_csv(inputFile, dtype={\"primary_price\": \"string\"})\n",
    "df = df.drop(['id'], axis=1)\n",
    "#df = df[0:10000]\n",
    "print(\"size=\", df.shape)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define several helper methods for preprocessing our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize Normalization\n",
    "def normalize(tokens):\n",
    "    lem = WordNetLemmatizer()\n",
    "    return [lem.lemmatize(token, pos=get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "# map NLTKâ€™s POS tags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "# tokenize, remove stop words and numbers\n",
    "def standardize_text_v2(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    removals = digits + string.punctuation\n",
    "    table = str.maketrans('', '', removals)\n",
    "    words = [w.translate(table) for w in tokens]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    remove_list = [',', '(', ')', \"'\", '\"', ' ', \"'s\", 'nan', \"''\", ',', '']\n",
    "    words = [token for token in words if not token in remove_list]\n",
    "    words = normalize(words)\n",
    "    return words\n",
    "\n",
    "# tokenize, lower, removes \"http\" , \"https\", \"www\"\n",
    "def standardize_text_v3(text):\n",
    "    removals = [\"http\", \"https\", \"www\", \"/\"]\n",
    "    for item in removals:\n",
    "        text = text.replace(\"item\", \"\")\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "    return tokens\n",
    "\n",
    "# selects k random records and check their prediction manually\n",
    "def write_random_records(my_df,file_name, category=None):\n",
    "\n",
    "    if category is not None:\n",
    "        my_df = my_df[my_df['class'] == category]\n",
    "        file_name = file_name + \"_category_\"\n",
    "\n",
    "    random_records = random.sample(my_df.index.to_list(), k=Globals.sample_test_size)\n",
    "    test = my_df.loc[\n",
    "        random_records, ['class', 'labels', 'product_type', 'full_store_product_url', 'all_text_original']]\n",
    "\n",
    "    test.to_csv(\"../data/validate/test_random_unseen_data_\" + file_name + \".csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply these methods to the columns we are interested. We do not want to remove stop words from Vendor name and website urls, hence we use standardize_text_v3 method and we use standardize_text_v2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from string import digits\n",
    "\n",
    "\n",
    "df['title'] = df['title'].astype(str)\n",
    "df['product_type'] = df['product_type'].astype(str)\n",
    "df['description'] = df['description'].astype(str)\n",
    "df['store_domain'] = df['store_domain'].astype(str)\n",
    "df['vendor_name'] = df['vendor_name'].astype(str)\n",
    "df['store_product_brand_domain'] = df['store_product_brand_domain'].astype(str)\n",
    "df['vendor_name_original'] = df['vendor_name'].str.lower() #we need it later\n",
    "\n",
    "df['product_type'] = df['product_type'].apply(standardize_text_v2)\n",
    "df['vendor_name'] = df['vendor_name'].apply(standardize_text_v3)\n",
    "df['store_domain'] = df['store_domain'].apply(standardize_text_v3)\n",
    "df['description'] = df['description'].apply(standardize_text_v2)\n",
    "df['title'] = df['title'].apply(standardize_text_v2)\n",
    "df['store_product_brand_domain'] = df['store_product_brand_domain'].apply(standardize_text_v2)\n",
    "\n",
    "# combine all token in all columns into one column\n",
    "df['all_tokens'] = df.values[:, 0:6].sum(axis=1)\n",
    "df['all_text'] = df['all_tokens'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4458\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    " \n",
    "non_english=[]\n",
    "count=0\n",
    "for row in range(0,df.shape[0]):\n",
    "    str1=\" \"\n",
    "    text =  str1.join(df.loc[row,'title'])\n",
    "    text +=  str1.join(df.loc[row,'product_type'])\n",
    "    if  not text :\n",
    "        text =   df.loc[row,'all_text']\n",
    "    if( detect(text) != 'en'):\n",
    "        #print (row,\"\\t\",text)\n",
    "        count+=1\n",
    "        non_english.append(row)\n",
    "print (count)\n",
    "\n",
    "df=df.drop(non_english)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Heuristics to Label our Unlabeled Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we no labelled records, we need to label some records. To achieve this, we have three options: 1-manually label some records, 2-prepare some heuristics to automate this job for us 3- use Amazon Sagemaker, Mechanical Turk or any other crowd sourcing websites \n",
    "\n",
    "In this tutorial, we prepare heuristics in python to label some records, so we can use them for training our classifiers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Phase 1\n",
    "\n",
    "One simple heuristics which we can use is to count the exact keywords of our classes (baby, men, women, kid, unisex) or their synonyms in the product information, choose the label with the maximum count of occurrence.  So, in this step we define our classes and their synonyms and we look for their occurrence in our product information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our categories and their related words\n",
    "classes = ['unisex', 'men', 'women', 'kid', 'baby']\n",
    "manNet = ['man', 'men', 'male', 'gentleman', 'gent', 'masculine', ' manlike', ' mannish']\n",
    "womanNet = ['woman', 'women', 'lady', 'female', 'ladies', 'girl', 'feminine']\n",
    "babyNet = ['baby', 'toddler', 'infant', 'babe', 'bambino', 'infant', 'newborn']\n",
    "kidNet = ['kid', 'child', 'children', 'child', 'youth', 'joni', 'schoolchild', 'schoolgirl', 'schoolkid', 'junior']\n",
    "unisexNet = ['unisex', 'androgynous', 'genderless', 'unisexual']\n",
    "all_Nets_list = [manNet, womanNet, babyNet, kidNet, unisexNet]\n",
    "\n",
    "\n",
    "# count occurence of keywords in the list\n",
    "def countFreq(product_info, keywordList):\n",
    "    total_count = 0\n",
    "    for item in product_info:\n",
    "        total_count += keywordList.count(item)\n",
    "    return total_count\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we count the occurrence of the keywords and select the label for each record according to the number occurrence of the keywords. For the records which we could not find any keyword, we set class to -1. So, class =-1 \n",
    "returns us unlabeled records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which feature to focus for labeling\n",
    "imp_feature = 'product_type'\n",
    "\n",
    "df['unisex'] = df[imp_feature].apply(countFreq, keywordList=unisexNet)\n",
    "df['men'] = df[imp_feature].apply(countFreq, keywordList=manNet)\n",
    "df['women'] = df[imp_feature].apply(countFreq, keywordList=womanNet)\n",
    "df['baby'] = df[imp_feature].apply(countFreq, keywordList=babyNet)\n",
    "df['kid'] = df[imp_feature].apply(countFreq, keywordList=kidNet)\n",
    "df['class'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the label with the highest occurance of the keyword\n",
    "def findLabel(row):\n",
    "    maxCount = max(row)\n",
    "    if maxCount > 0:\n",
    "        maxLabel = row[row == maxCount].index[0]\n",
    "    else:\n",
    "        maxLabel = '-1'\n",
    "    return maxLabel\n",
    "\n",
    "\n",
    "# encode the classes to their index\n",
    "df.loc[:, 'class'] = df.loc[:, classes].apply(findLabel, axis=1)\n",
    "\n",
    "# keep track of record labeled in this round\n",
    "labeled_data_index_r1 = df[df['class'] != '-1'].index.to_list()\n",
    "\n",
    "print(\"Number of records labeled in round 1:\", len(labeled_data_index_r1))\n",
    "print(\"Number of records not labeled in round 1:\", df.shape[0] - len(labeled_data_index_r1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the distribution of classes looks so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_class_distribution(data_frame, groupby_feature, class_name, starting_index=0):\n",
    "    grouped = data_frame.groupby([class_name])\n",
    "    values = grouped[groupby_feature].agg(np.size)[starting_index:]\n",
    "    labels = values.index.tolist()\n",
    "    y_pos = np.arange(len(labels))\n",
    "    plt.bar(y_pos, values)\n",
    "    plt.xticks(y_pos, labels)\n",
    "    plt.xlabel('Product categories')\n",
    "    plt.ylabel('Number of Products')\n",
    "    plt.show()\n",
    "    print(data_frame[class_name].value_counts())\n",
    "    \n",
    "plot_class_distribution(df, 'product_type', 'class', starting_index=1)\n",
    "\n",
    "# let's check the labeling of some randoms records\n",
    "test = df.loc[labeled_data_index_r1,:]\n",
    "write_random_records(df, file_name=\"Labelling-Phase1\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Phase 2\n",
    "\n",
    "Now let's add another heuristic to label more records. In this step. we can look for common words and choose the ones which can uniquely identify a class and search for those keyword to label more record.\n",
    "\n",
    "In this step, we assume that when an item is usually being used by one gender (for example, neckless is usually being worn by females), then a special type of neckless which is designed for men, will have men keyword in its description and has already being labelled in the previous round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "unisexProduct =['electronics','phone','fruit','movie','vegetable','seafood','ipad','video','music','book',\n",
    "                'dairy','egg','fridge', 'phone','supplement','cable','cookware','cook','novel','bike','headphone',\n",
    "                'appliance','battery','vitamin','fence','garden','speaker','camera','kitchen','radio','backpack',\n",
    "                'frozen','food','household','safety','sex toys','skate','tuna','home']\n",
    "\n",
    "womanProduct = ['jewellery','pregnancy','make up','nail polish','eye shadow','skirt','manicure','pedicure',\n",
    "                'jewellery','bracelet','necklace','earring','jewelry','lingerie']\n",
    "\n",
    "menProduct = ['shave', 'tuxedo']\n",
    "\n",
    "kidProduct = ['school', 'disney', 'spider', 'barbie', 'doll']\n",
    "\n",
    "babyProduct =  ['pacifier','stroller','diaper','potty','walker','playmat','car seat','lip liner','babyliss',\n",
    "                 'maternity','teether','nursery','carrier','crib','rattle','sleeper']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count the new keyword in the product information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# find the total frequency of a list of keywords in a tokenized list\n",
    "def count_occurance_keyword(tokenized_list, category_list):\n",
    "    count = 0\n",
    "    text_data = ' '.join(tokenized_list) + \" \"\n",
    "    for keyword in category_list:\n",
    "        count = text_data.count(keyword + \" \")\n",
    "    return count\n",
    "\n",
    "\n",
    "def findLabel_commonKeywords(dataFrame, feature):\n",
    "    count_unisex = count_occurance_keyword(dataFrame[feature], unisexProduct)\n",
    "    count_men = count_occurance_keyword(dataFrame[feature], menProduct)\n",
    "    count_woman = count_occurance_keyword(dataFrame[feature], womanProduct)\n",
    "    count_kid = count_occurance_keyword(dataFrame[feature], kidProduct)\n",
    "    count_baby = count_occurance_keyword(dataFrame[feature], babyProduct)\n",
    "\n",
    "    index = ['unisex', 'men', 'women', 'kid', 'baby']\n",
    "    counters = [count_unisex, count_men, count_woman, count_kid, count_baby]\n",
    "    frequency = pd.Series(counters, index=index)\n",
    "\n",
    "    # find label with maximum frequency\n",
    "    max_frequency = max(frequency)\n",
    "    max_label = frequency.idxmax() if max_frequency > 0 else '-1'\n",
    "    return max_label\n",
    "\n",
    "\n",
    "not_labled_index = df['class'] == '-1'\n",
    "df.loc[not_labled_index, 'class'] = df.loc[not_labled_index, :].apply(findLabel_commonKeywords, axis=1,\n",
    "                                                                      args=[imp_feature])\n",
    "\n",
    "# Find records labeled in  round 2\n",
    "# keep trackes of record labeled in this round\n",
    "labeled_data_index = df[df['class'] != '-1'].index.to_list()\n",
    "\n",
    "labeled_data_index_r2 = [i for i in labeled_data_index if i not in labeled_data_index_r1]\n",
    "\n",
    "\n",
    "print(\"Number of records labeled in round 2:\", len(labeled_data_index_r2))\n",
    "print(\"Number of records not labeled yet:\", df.shape[0] - len(labeled_data_index))\n",
    "\n",
    "plot_class_distribution(df, 'product_type', 'class', starting_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find most commmon words in product information which are not included in the categories lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories_lists = [unisexProduct, menProduct, womanProduct, kidProduct, babyProduct]\n",
    "all_keyword_lists = all_Nets_list + all_categories_lists\n",
    "\n",
    "all_keyword_set = set()\n",
    "for list_a in all_keyword_lists:\n",
    "    all_keyword_set.update(set(list_a))\n",
    "\n",
    "# combine all rows' tokens  into one list\n",
    "all_words = list([a for b in df['product_type'] for a in b])\n",
    "\n",
    "# Find the most frequent words which are not included  categories lists\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(all_words)\n",
    "for word, number in fdist.most_common(50):\n",
    "    if word not in all_keyword_set:\n",
    "        print(word, end=', ')\n",
    "        \n",
    "\n",
    "# let's check the labeling of some randoms records\n",
    "test = df.loc[labeled_data_index_r2,:]\n",
    "write_random_records(df, file_name=\"Labelling-Phase2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these keyword can be used to uniquely recognize a category, so let's move."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeling Phase 3\n",
    "If products from a vendor all belong to one particular category (given that at least 10 products are listed, we can assign that category to other products from the same vendor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "homo_brands = {}\n",
    "labeled_data = df[df['class'] != '-1'].copy()\n",
    "grouped = labeled_data.groupby(['vendor_name_original'])\n",
    "for key, group in grouped:\n",
    "    class_group = grouped.get_group(key).groupby(['class'])\n",
    "\n",
    "    # if all products belong to one category\n",
    "    if len(class_group) == 1:\n",
    "        # If at least 10 products are listed for a company\n",
    "        if (class_group['class'].count()[0]) > 10:\n",
    "            homo_brands[key] = list(class_group.groups.keys())[0]\n",
    "\n",
    "\n",
    "homo_vendor_bool = df['vendor_name_original'].apply(lambda x: x in list(homo_brands.keys()))\n",
    "not_labled_bool = df['class'] == '-1'\n",
    "\n",
    "# records which are not labeled yet and belong to homo vendor\n",
    "homo_notLabeld_bool = np.logical_and(not_labled_bool, homo_vendor_bool)\n",
    "homo_notLabeld_index = df[homo_notLabeld_bool].index\n",
    "\n",
    "pd.DataFrame({'homo_vendor': homo_vendor_bool, 'not_labled_bool': not_labled_bool,\n",
    "              'homo_notLabeld_bool': homo_notLabeld_bool})\n",
    "\n",
    "def get_homo_class(x):\n",
    "    vendor = x['vendor_name_original']\n",
    "    return homo_brands[vendor]\n",
    "\n",
    "df.loc[homo_notLabeld_index, 'class'] = df.loc[homo_notLabeld_index, :].apply(get_homo_class, axis=1)\n",
    "\n",
    "# keep track of record labeled in round 3\n",
    "labeled_data_index = df[df['class'] != '-1'].index.to_list()\n",
    "labeled_data_index_before = labeled_data_index_r1 + labeled_data_index_r2\n",
    "labeled_data_index_r3 = [i for i in labeled_data_index if i not in labeled_data_index_before]\n",
    "\n",
    "print(\"Number of records labeled in round 3:\", len(labeled_data_index_r3))\n",
    "print(\"Number of records not labeled yet:\", df.shape[0] - len(labeled_data_index))\n",
    "\n",
    "plot_class_distribution(df, 'product_type', 'class', starting_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save the results, so we do not need to repeat labelling process every time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('../data/labeled/labeled_dataV3', 'wb')\n",
    "pickle.dump(df, dbfile)\n",
    "dbfile.close()\n",
    "\n",
    "\n",
    "# let's check the labeling of some randoms records\n",
    "test = df.loc[labeled_data_index_r31,:]\n",
    "write_random_records(df, file_name=\"Labelling-Phase3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight] *",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
