{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import keras\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import codecs\n",
    "import gc\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pivottablejs import pivot_ui\n",
    "\n",
    "gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputFile=\"../data/cleaned/outputFile2-0-500000.csv\" \n",
    "df = pd.read_csv(inputFile, dtype = {\"primary_price\" : \"string\"})\n",
    "df = df.drop(['id'], axis=1)\n",
    "#df=df[0:500000]\n",
    "\n",
    "# our categories and their related words\n",
    "classes=['unisex', 'men', 'women', 'kid', 'baby']\n",
    "manNet=['man','men', 'male', 'gentleman', 'gent','masculine',' manlike', ' mannish']\n",
    "womanNet=['woman', 'women', 'lady', 'female' 'ladies', 'girl','feminine','unmacho','metrosexual' ]\n",
    "babyNet= ['baby','toddler','infant', 'babe', 'bambino', 'infant', 'neonate', 'newborn']\n",
    "kidNet=['kid','child', 'children', 'child', 'youth', 'joni','schoolchild', 'schoolgirl', 'schoolkid','junior']\n",
    "unisexNet=['unisex','androgynous', 'genderless', 'unisexual']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def plot_class_distribution (data_frame, groupby_feature,class_name, starting_index=0):\n",
    "    grouped  = data_frame.groupby([class_name]) \n",
    "    values=grouped[groupby_feature].agg(np.size)[starting_index:]  \n",
    "    labels =  values.index.tolist()  \n",
    "    y_pos = np.arange(len(labels))\n",
    "    plt.bar(y_pos, values)\n",
    "    plt.xticks(y_pos, labels)\n",
    "    plt.xlabel('Product categories')\n",
    "    plt.ylabel('Number of Products')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the dataset\n",
    "\n",
    "\n",
    "#map NLTKâ€™s POS tags\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "#Lemmatize Normalization\n",
    "def normalize (tokens):\n",
    "    lem = WordNetLemmatizer()\n",
    "    return [lem.lemmatize(token,pos=get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "#clean up  tokenized  data\n",
    "def standardize_tokens (tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "#Few regular expressions to clean up  text data\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\".com\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"nan\", \"\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    #tokenize text\n",
    "    df[text_field] = df[text_field].apply(word_tokenize)\n",
    "    #remove stop words\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    df[text_field] = df[text_field].apply(lambda x: [item for item in x if item not in stop_words] )\n",
    "    #normalize text\n",
    "    df[text_field] = df[text_field].apply(normalize)\n",
    "        \n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_type</th>\n",
       "      <th>vendor_name</th>\n",
       "      <th>title</th>\n",
       "      <th>store_domain</th>\n",
       "      <th>store_product_brand_domain</th>\n",
       "      <th>description</th>\n",
       "      <th>primary_price</th>\n",
       "      <th>full_store_product_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cookware</td>\n",
       "      <td>Hestan</td>\n",
       "      <td>Hestan Nanobond Stainless 3qt/2.8L covered sau...</td>\n",
       "      <td>atlantagrillcompany.com</td>\n",
       "      <td>hestanculinary.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>290.0</td>\n",
       "      <td>https://atlantagrillcompany.com/products/hesta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cookware</td>\n",
       "      <td>Hestan</td>\n",
       "      <td>Hestan Nanobond Titanium Stockpot 8-Quart</td>\n",
       "      <td>atlantagrillcompany.com</td>\n",
       "      <td>hestanculinary.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>450.0</td>\n",
       "      <td>https://atlantagrillcompany.com/products/hesta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Skillets</td>\n",
       "      <td>Hestan</td>\n",
       "      <td>Hestan Nanobond 8.5\" Open Skillet</td>\n",
       "      <td>dasallas.com</td>\n",
       "      <td>hestanculinary.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>189.95</td>\n",
       "      <td>https://dasallas.com/products/hestan-nanobond-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Saucepans &amp; Sauciers</td>\n",
       "      <td>Hestan</td>\n",
       "      <td>Hestan Nanobond 5-Quart Covered Essential Pan ...</td>\n",
       "      <td>dasallas.com</td>\n",
       "      <td>hestanculinary.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>499.95</td>\n",
       "      <td>https://dasallas.com/products/hestan-nanobond-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cookware</td>\n",
       "      <td>Hestan</td>\n",
       "      <td>Hestan ProBond Forged Stainless Steel Ultimate...</td>\n",
       "      <td>atlantagrillcompany.com</td>\n",
       "      <td>hestanculinary.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>799.99</td>\n",
       "      <td>https://atlantagrillcompany.com/products/hesta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           product_type vendor_name  \\\n",
       "0              Cookware      Hestan   \n",
       "1              Cookware      Hestan   \n",
       "2              Skillets      Hestan   \n",
       "3  Saucepans & Sauciers      Hestan   \n",
       "4              Cookware      Hestan   \n",
       "\n",
       "                                               title             store_domain  \\\n",
       "0  Hestan Nanobond Stainless 3qt/2.8L covered sau...  atlantagrillcompany.com   \n",
       "1          Hestan Nanobond Titanium Stockpot 8-Quart  atlantagrillcompany.com   \n",
       "2                  Hestan Nanobond 8.5\" Open Skillet             dasallas.com   \n",
       "3  Hestan Nanobond 5-Quart Covered Essential Pan ...             dasallas.com   \n",
       "4  Hestan ProBond Forged Stainless Steel Ultimate...  atlantagrillcompany.com   \n",
       "\n",
       "  store_product_brand_domain  description primary_price  \\\n",
       "0         hestanculinary.com          NaN         290.0   \n",
       "1         hestanculinary.com          NaN         450.0   \n",
       "2         hestanculinary.com          NaN        189.95   \n",
       "3         hestanculinary.com          NaN        499.95   \n",
       "4         hestanculinary.com          NaN        799.99   \n",
       "\n",
       "                              full_store_product_url  \n",
       "0  https://atlantagrillcompany.com/products/hesta...  \n",
       "1  https://atlantagrillcompany.com/products/hesta...  \n",
       "2  https://dasallas.com/products/hestan-nanobond-...  \n",
       "3  https://dasallas.com/products/hestan-nanobond-...  \n",
       "4  https://atlantagrillcompany.com/products/hesta...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['title']=df['title'].astype(str)\n",
    "df['product_type']=df['product_type'].astype(str)\n",
    "df['description']=df['description'].astype(str)\n",
    "df['store_domain']=df['store_domain'].astype(str)\n",
    "df['vendor_name']=df['vendor_name'].astype(str)\n",
    "df['store_product_brand_domain']=df['store_product_brand_domain'].astype(str)\n",
    " \n",
    "# df['all_text_original'] = df.values.sum(axis=1)\n",
    "#df['all_text_original'] = df.apply( str.join( \" \" ))\n",
    "\n",
    "df['all_text_original'] = df['product_type']+\" \"+df['vendor_name']+\" \"+df['title']+\" \"+df['store_domain']+\" \"+df['store_product_brand_domain']+\" \"+df['description']\n",
    "df['vendor_name_original'] = df['vendor_name'].str.lower()\n",
    "\n",
    "standardize_text(df, 'product_type')\n",
    "standardize_text(df, 'vendor_name')\n",
    "standardize_text(df, 'title')\n",
    "standardize_text(df, 'store_domain')\n",
    "standardize_text(df, 'description')\n",
    "standardize_text(df,'store_product_brand_domain')\n",
    "\n",
    "#combine all token in all columns into one column\n",
    "#df['all_tokens'] = df.values[:,['product_type','vendor_name', 'title', 'store_domain', 'description']].sum(axis=1)\n",
    "df['all_tokens'] = df.values[:,0:6].sum(axis=1)\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Frequency of the categories and their synonyms in the product information \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count occurence of keyword in the list\n",
    "def countFreq (product_info, keywordList):\n",
    "    total_count =0\n",
    "    for item in product_info:\n",
    "      total_count += keywordList.count(item)\n",
    "    return total_count\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "df['unisex'] = df['all_tokens'].apply(countFreq,keywordList=unisexNet)   \n",
    "df['men'] = df['all_tokens'].apply(countFreq,keywordList=manNet) \n",
    "df['women'] = df['all_tokens'].apply(countFreq,keywordList=womanNet)  \n",
    "df['baby'] = df['all_tokens'].apply(countFreq,keywordList=babyNet)  \n",
    "df['kid'] = df['all_tokens'].apply(countFreq,keywordList=kidNet)\n",
    "  \n",
    "df['class']=  '-1'\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the label with the highest occurance of the keyword "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose the label with the highest occurance of the keyword \n",
    "def findLabel(dataFrame): \n",
    "        maxCount=max(dataFrame)\n",
    "        if maxCount> 0:\n",
    "            maxLabel= dataFrame[dataFrame == maxCount].index[0] \n",
    "        else:\n",
    "            maxLabel='-1'\n",
    "        return maxLabel\n",
    "    \n",
    "df.loc[:,'class']=df.loc[:,classes].apply(findLabel,axis=1) \n",
    "display (df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The distribution of classes including Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped  = df.groupby(['class'])     \n",
    "print(grouped['product_type'].agg(np.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution (df, 'product_type','class', starting_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The distribution of classes excluding Nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution (df, 'product_type','class', starting_index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most common keywords in product information and label records containg those keywords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1) Define commmon keywords for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unisexProduct= ['electronics', 'phone','fruit','movie','vegetable', \n",
    "                'seafood','ipad','video','music','book', 'dairy',\n",
    "                'egg', 'fridge', 'phone', 'supplement', 'cable', \n",
    "                'cookware', 'cook', 'novel', 'bike','headphone',\n",
    "               'appliance','battery', 'vitamin','fence', 'garden',\n",
    "                'speaker','camera', 'kitchen', 'radio', 'backpack'\n",
    "                'frozen', 'food', 'household', 'safety', 'skate' ]\n",
    "\n",
    "womanProduct=['jewellery', 'pregnancy', 'make up', 'nail polish', \n",
    "              'eye shadow', 'skirt','Manicure', 'Pedicure'   ]\n",
    "\n",
    "menProduct= ['shave', 'tuxedo', 'tie'] \n",
    "\n",
    "\n",
    "kidProduct=['school', 'disney', 'spider', 'barbie','doll'  ]\n",
    "\n",
    "babyProduct=['Pacifier' ,'Strollers', 'diapers', 'potty', 'walkers',\n",
    "             'playmat',  'Car Seat', 'lip liner', 'Babyliss', 'maternity', \n",
    "             'Teether', 'nursery', 'carrier', 'crib', 'Rattle', 'sleeper']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lemmitize and standardize the all the categories lists\n",
    "unisexProduct=normalize(unisexProduct)\n",
    "unisexProduct=standardize_tokens(unisexProduct)\n",
    "menProduct=normalize(menProduct)\n",
    "menProduct=standardize_tokens(menProduct)\n",
    "womanProduct=normalize(womanProduct)\n",
    "womanProduct=standardize_tokens(womanProduct)\n",
    "kidProduct=normalize(kidProduct)\n",
    "kidProduct=standardize_tokens(kidProduct) \n",
    "babyProduct=normalize(babyProduct)\n",
    "babyProduct=standardize_tokens(babyProduct) \n",
    "\n",
    "all_categories_lists=[unisexProduct,   menProduct ,  womanProduct ,  kidProduct ,  babyProduct]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Find most commmon words in product information which are not included in the categories lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Let's inspect word and vocabulary of our data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all rows' tokens  into one list\n",
    "all_words=list([a for b in df['all_tokens'].tolist() for a in b])\n",
    "all_words=list(filter(lambda a: a not in [',', '(', ')'], all_words))\n",
    "\n",
    "\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most commmon words in product_type\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(all_words)\n",
    "print(fdist)\n",
    "fdist.most_common(10)\n",
    "\n",
    "# Frequency Distribution Plot\n",
    "import matplotlib.pyplot as plt\n",
    "fdist.plot(20,cumulative=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most frequent words which are not included  categories lists\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(all_words)\n",
    " \n",
    "for word,number in fdist.most_common(30):\n",
    "    if word not in all_categories_lists:\n",
    "        print (word)\n",
    "        #pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Label records containg common keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the total frequency of a list of keywords in a tokenized list \n",
    "def count_occurance_keyword ( tokenized_list, category_list ) :\n",
    "        count=0\n",
    "        text_data= ' '.join( tokenized_list)+\" \"\n",
    "        print(text_data)\n",
    "        for keyword in category_list:\n",
    "                count = text_data.count(keyword+\" \") \n",
    "                print(keyword, count)\n",
    "        return count     \n",
    "    \n",
    "def findLabel_commonKeywords(dataFrame, feature): \n",
    "    count_unisex = count_occurance_keyword(dataFrame[feature], unisexProduct) \n",
    "    count_men   =  count_occurance_keyword(dataFrame[feature], menProduct) \n",
    "    count_woman  = count_occurance_keyword(dataFrame[feature], womanProduct) \n",
    "    count_kid    = count_occurance_keyword(dataFrame[feature], kidProduct) \n",
    "    count_baby   = count_occurance_keyword(dataFrame[feature], babyProduct) \n",
    "    \n",
    "    index=['unisex', 'men', 'women', 'kid', 'baby']\n",
    "    counters=[count_unisex, count_men, count_woman,count_kid,count_baby]\n",
    "    frequency= pd.Series(counters, index=index)\n",
    "    \n",
    "    # find label with maximum frequency\n",
    "    max_frequency= max(frequency)\n",
    "    max_label= frequency.idxmax() if max_frequency> 0 else '-1' \n",
    "    return max_label\n",
    "\n",
    "\n",
    "#notLabled = df[ df['class'] == '-1' ]  \n",
    "not_labled_index=  df['class'] == '-1' \n",
    "df.loc[not_labled_index, 'class'] = df.loc[not_labled_index,:].apply(findLabel_commonKeywords, axis=1,args=['all_tokens']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ignore, just to test funstion\n",
    "#count_occurance_keyword(['tie','men', 'baby', 'shaver','shaves', 'tie','shave', 'tiered'], ['tie', 'shave']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(  df[ df['class'] != '-1' ].head() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_class_distribution (df, 'product_type','class', starting_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df['class'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_text']= df['all_tokens'].apply(lambda x: \" \".join(x))\n",
    "df.to_csv(\"../data/labeled/labeled_dataV1.csv\", index=True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeling more records based on vendor names\n",
    "\n",
    "If products from a vendor all belong to one particular category (given that at least 10 products are listed), we can assign that category to other products from the same vendor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivot_ui(labeled_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_brands= {}\n",
    "labeled_data=df[df['class']!= '-1'].copy()\n",
    "grouped  = labeled_data.groupby(['vendor_name_original'])\n",
    "for key, group in grouped:\n",
    "   class_group = grouped.get_group(key).groupby(['class']) \n",
    "   #print(key, len(class_group ), class_group['class'].count() )\n",
    "   # if all products belong to one category\n",
    "   if len(class_group)==1:\n",
    "      # If at least 10 products are listed for a company\n",
    "      if  (class_group['class'].count()[0]) > 10 :\n",
    "         homo_brands[key]= list(class_group.groups.keys())[0]\n",
    "\n",
    "print(homo_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_vendor_bool=  df['vendor_name_original'].apply( lambda x: x in list(homo_brands.keys())) \n",
    "not_labled_bool=   df['class'] == '-1'\n",
    "#records which are not labeled yet and belong to homo vendor\n",
    "homo_notLabeld_bool=np.logical_and(not_labled_bool, homo_vendor_bool)\n",
    "homo_notLabeld_index= df[homo_notLabeld_bool].index\n",
    "\n",
    "pd.DataFrame({'homo_vendor': homo_vendor_bool, 'not_labled_bool': not_labled_bool,'homo_notLabeld_bool':homo_notLabeld_bool  })\n",
    "display(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"not labeled \\n\",df[not_labled_bool].index)\n",
    "print(\"homo \\n\",df[homo_vendor_bool].index)\n",
    "print(\"homo and notlabeled \\n\",df[homo_notLabeld_bool].index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['class'].value_counts())\n",
    "\n",
    "def get_homo_class ( x ):\n",
    "    vendor=x['vendor_name_original']\n",
    "    #print(vendor, homo_brands[vendor])\n",
    "    return homo_brands[vendor]\n",
    "    \n",
    "df.loc[homo_notLabeld_index, 'class'] = df.loc[homo_notLabeld_index,:].apply(get_homo_class, axis=1)\n",
    "print(df['class'].value_counts())\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Labeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['all_text']= df['all_tokens'].apply(lambda x: \" \".join(x))\n",
    "df.to_csv(\"../data/labeled/labeled_dataV2.csv\", index=True)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(labeled_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
